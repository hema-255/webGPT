{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPm1nttKFitwookN3nT4KHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hema-255/webGPT/blob/main/webGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok sentence-transformers faiss-cpu -q"
      ],
      "metadata": {
        "id": "M7HKiZQLF9V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPIZOt3kDF1Q"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install requests beautifulsoup4 scrapy pinecone-client -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language_tool_python -q"
      ],
      "metadata": {
        "id": "1c6jTQsoXEJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape content from a given URL\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract all text content\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = [para.get_text(strip=True) for para in paragraphs]\n",
        "        return content\n",
        "    else:\n",
        "        print(f\"Failed to fetch {url}: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "urls = [\n",
        "    \"https://byjus.com/biology/nutrition-in-plants/\",\n",
        "    \"https://byjus.com/biology/nutrition-modes-living-organisms/\",\n",
        "    \"https://byjus.com/biology/nutrition-animals/\",\n",
        "    \"https://byjus.com/biology/photosynthesis/\"\n",
        "]\n",
        "\n",
        "# Scrape content from all URLs\n",
        "website_data = {url: scrape_website(url) for url in urls}"
      ],
      "metadata": {
        "id": "7DWOBQ9HDvBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split text into chunks\n",
        "def chunk_text(content, max_length=500):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for paragraph in content:\n",
        "        if current_length + len(paragraph) > max_length:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(paragraph)\n",
        "        current_length += len(paragraph)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Chunk the scraped content\n",
        "chunked_data = {url: chunk_text(content) for url, content in website_data.items()}"
      ],
      "metadata": {
        "id": "AcWWL-pTEvYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the chunks\n",
        "embeddings = {}\n",
        "for url, chunks in chunked_data.items():\n",
        "    embeddings[url] = embedding_model.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "print(\"Embeddings generated successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU614OJdGq9N",
        "outputId": "da7bb910-70d5-40f0-f43b-d71e7553e339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = embeddings[urls[0]][0].shape[0]  # Embedding dimension\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
        "\n",
        "# Add embeddings to the FAISS index\n",
        "chunk_metadata = []  # To track metadata for each chunk\n",
        "for url, embed_vectors in embeddings.items():\n",
        "    # Move embeddings to CPU and convert to NumPy\n",
        "    embed_vectors_np = np.array([vec.cpu().numpy() for vec in embed_vectors])\n",
        "    index.add(embed_vectors_np)  # Add to FAISS index\n",
        "    chunk_metadata.extend([(url, i) for i in range(len(embed_vectors_np))])\n",
        "\n",
        "print(f\"FAISS index contains {index.ntotal} vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meYhL1pxIFcc",
        "outputId": "b8e7c664-0b59-4a01-81b9-7941d5e79abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index contains 56 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle user queries\n",
        "def query_rag_system(query, top_k=3):\n",
        "    # Convert query into embedding\n",
        "    query_vector = embedding_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "    # Perform similarity search\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "\n",
        "    # Retrieve relevant chunks\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        url, chunk_id = chunk_metadata[idx]\n",
        "        results.append((url, chunked_data[url][chunk_id]))\n",
        "    return results"
      ],
      "metadata": {
        "id": "PjRTKnpgJeg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "user_query = \"What is Chrolophyll?\"\n",
        "retrieved_chunks = query_rag_system(user_query)\n",
        "for url, chunk in retrieved_chunks:\n",
        "    print(f\"From {url}:\\n{chunk}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNpMKFZyXF4O",
        "outputId": "f6ba1931-6cfd-4b97-bbbc-039ab0686be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://byjus.com/biology/photosynthesis/:\n",
            "Chlorophyll is a green pigment found in the chloroplasts of theplant celland in the mesosomes of cyanobacteria. This green colour pigment plays a vital role in the process of photosynthesis by permitting plants to absorb energy from sunlight. Chlorophyll is a mixture of chlorophyll-aand chlorophyll-b.Besides green plants, other organisms that perform photosynthesis contain various other forms of chlorophyll such as chlorophyll-c1,  chlorophyll-c2,  chlorophyll-dand chlorophyll-f.\n",
            "\n",
            "From https://byjus.com/biology/nutrition-in-plants/:\n",
            "Chlorophyll is a green pigment present in leaves which helps the leaves capture energy from sunlight to prepare their food. This production of food which takes place in the presence of sunlight is known as photosynthesis. Hence, the sun serves as the primary source for all living organisms During photosynthesis, water and carbon dioxide are used in the presence of sunlight to produce carbohydrates and oxygen. Photosynthesis provides food to all living beings.\n",
            "\n",
            "From https://byjus.com/biology/photosynthesis/:\n",
            "Chloroplasts are the sites of photosynthesis in plants and blue-green algae.  All green parts of a plant, including the green stems, green leaves,  and sepals – floral parts comprise of chloroplasts – green colour plastids. These cell organelles are present only in plant cells and are located within the mesophyll cells of leaves. Photosynthesis process requires several factors such as: Also Read:Photosynthesis Early Experiments\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UI Starts here**"
      ],
      "metadata": {
        "id": "BkXfyrR4F637"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save FAISS index, metadata, and chunked data\n",
        "with open(\"faiss_index.pkl\", \"wb\") as f:\n",
        "    pickle.dump((index, chunk_metadata, chunked_data), f)\n",
        "\n",
        "print(\"FAISS index, metadata, chunked data saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCe6dUXfJJ5Q",
        "outputId": "9b43b92e-0970-41bd-e387-25aeb10e5389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index, metadata, chunked data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the FAISS index, metadata, and chunked data\n",
        "with open(\"faiss_index.pkl\", \"rb\") as f:\n",
        "    index, metadata, chunked_data = pickle.load(f)\n",
        "\n",
        "# Load the embedding model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to retrieve relevant chunks\n",
        "def retrieve_chunks(query, top_k=3):\n",
        "    query_vector = model.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vector), top_k)\n",
        "    results = [(metadata[i][0], chunked_data[metadata[i][0]][metadata[i][1]]) for i in indices[0]]\n",
        "    return results\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"WebGPT\")\n",
        "st.markdown(\"Ask me anything about the ingested content!\")\n",
        "\n",
        "# Input from user\n",
        "user_query = st.text_input(\"Enter your question:\", \"\")\n",
        "\n",
        "def beautify_answer(response):\n",
        "    justified_text = f'<div style=\"text-align: justify; font-size: 16px; line-height: 1.6;\">{response}</div>'\n",
        "    return justified_text\n",
        "\n",
        "\n",
        "if user_query:\n",
        "    st.markdown(\"### Retrieved Context:\")\n",
        "    retrieved_chunks = retrieve_chunks(user_query)\n",
        "    for i, (url, chunk) in enumerate(retrieved_chunks):\n",
        "        st.write(f\"**Source {i+1}:** {url}\")\n",
        "        st.markdown(beautify_answer(chunk), unsafe_allow_html=True)\n",
        "        st.write(\"\\n\")\n",
        "\n",
        "    # Generate response (basic concatenation for now)\n",
        "    response = \" \".join([str(chunk) for url, chunk in retrieved_chunks])  # Ensure only chunks are concatenated\n",
        "    # Fallback if no valid chunks\n",
        "    if not response.strip():\n",
        "      response = \"Sorry, I couldn't find relevant information to answer your query.\"\n",
        "    st.markdown(\"### Answer:\")\n",
        "    st.markdown(beautify_answer(response), unsafe_allow_html=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlOkFcS5K9BL",
        "outputId": "b3dfe9de-8291-4133-f8c6-8a591ef1e6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2qQez0La2vpg4sWLmlwlZCATVLQ_2S2WdARFXoJYQj5hGHYU8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivxMFJTJMYyK",
        "outputId": "d2471730-899d-44e9-e54c-51fd7e99a2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8501, \"http\")  # Specify port and protocol\n",
        "print(f\"Streamlit app is running at {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy01Sg0rLZGT",
        "outputId": "288a1c5f-4b5d-4f34-c936-607208311503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at NgrokTunnel: \"https://ef64-34-34-74-9.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"streamlit run app.py &\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZwWRpvmO8Z6",
        "outputId": "9b43bab7-ccb7-42d9-c5ce-70f45a8e449d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!lsof -i:8501\n",
        "\n",
        "!kill -9 PID"
      ],
      "metadata": {
        "id": "O0Y9lj9zNpm-"
      }
    }
  ]
}